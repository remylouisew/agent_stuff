{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUnquPUgUTex",
        "outputId": "fc35358a-8b07-43f3-a261-30129183fa72"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade google-genai google-api-python-client google-auth-httplib2 google-auth-oauthlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_Sca2oLyWi5"
      },
      "source": [
        "#Introduction: Building a Newsletter with a Team of AI Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twrrvkdh4GNi",
        "outputId": "1d38988c-6f7f-417d-a0ec-bdf069446f52"
      },
      "outputs": [],
      "source": [
        "!export GOOGLE_CLOUD_PROJECT='remy-sandbox'\n",
        "!export GOOGLE_CLOUD_LOCATION=global\n",
        "!export GOOGLE_GENAI_USE_VERTEXAI=True\n",
        "\n",
        "!gcloud config set project remy-sandbox\n",
        "\n",
        "!gcloud auth application-default login --scopes=openid,https://www.googleapis.com/auth/userinfo.email,https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/drive.readonly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbW3PA0S5fS0"
      },
      "outputs": [],
      "source": [
        "!export GOOGLE_CLOUD_PROJECT='remy-sandbox'\n",
        "!export GOOGLE_CLOUD_LOCATION=global\n",
        "!export GOOGLE_GENAI_USE_VERTEXAI=True\n",
        "\n",
        "\n",
        "client = genai.Client(vertexai=True, project='remy-sandbox', location='us-central1', http_options=HttpOptions(api_version=\"v1\"))\n",
        "MODEL_ID = \"gemini-2.5-flash\"\n",
        "\n",
        "def model_response(text,model_id):\n",
        "   response = client.models.generate_content(\n",
        "    model=genai.GenerativeModel(model_id),\n",
        "    contents = text\n",
        ")\n",
        "   return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDRH9UNcUZcm"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "import json\n",
        "from IPython.display import display, HTML, Markdown\n",
        "from google.genai.types import HttpOptions\n",
        "\n",
        "\n",
        "def show_json(obj):\n",
        "  print(json.dumps(obj.model_dump(exclude_none=True), indent=2))\n",
        "  return json.dumps(obj.model_dump(exclude_none=True), indent=2)\n",
        "\n",
        "def show_parts(r):\n",
        "  parts = r.candidates[0].content.parts\n",
        "  if parts is None:\n",
        "    finish_reason = r.candidates[0].finish_reason\n",
        "    print(f'{finish_reason=}')\n",
        "    return\n",
        "  for part in r.candidates[0].content.parts:\n",
        "    if part.text:\n",
        "      display(Markdown(part.text))\n",
        "      output = part.text\n",
        "    elif part.executable_code:\n",
        "      display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n",
        "      output = part.executable_code\n",
        "    else:\n",
        "      show_json(part)\n",
        "\n",
        "  grounding_metadata = r.candidates[0].grounding_metadata\n",
        "  if grounding_metadata and grounding_metadata.search_entry_point:\n",
        "    display(HTML(grounding_metadata.search_entry_point.rendered_content))\n",
        "  return output\n",
        "\n",
        "!export GOOGLE_CLOUD_PROJECT='remy-sandbox'\n",
        "!export GOOGLE_CLOUD_LOCATION=global\n",
        "!export GOOGLE_GENAI_USE_VERTEXAI=True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#client = genai.Client(api_key=\"\")\n",
        "\n",
        "client = genai.Client(vertexai=True, project='remy-sandbox', location='us-central1', http_options=HttpOptions(api_version=\"v1\"))\n",
        "MODEL_ID = \"gemini-2.5-flash\"\n",
        "\n",
        "def model_response(text,model_id):\n",
        "   response = client.models.generate_content(\n",
        "    model=model_id,\n",
        "    contents = text\n",
        ")\n",
        "   return response.text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqPLD4ggyQ9H"
      },
      "source": [
        "##Creating the search agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC04YihLyQTy"
      },
      "outputs": [],
      "source": [
        "search_tool = {'google_search': {}}\n",
        "search_chat = client.chats.create(model=\"gemini-2.5-flash\", config={'tools': [search_tool]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iULV58VxXRiO"
      },
      "source": [
        "#Create the Google Drive Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VTKZ9n4Xq9q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Optional\n",
        "from datetime import datetime\n",
        "from dateutil import parser as dateparser\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from google.auth import default\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.credentials import Credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9QFlQT-tkP-",
        "outputId": "b5e173bf-01bc-4f45-c6ef-0f24c4877769"
      },
      "outputs": [],
      "source": [
        "# gemini_gdrive_tool.py\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.auth import default\n",
        "from googleapiclient.discovery import build\n",
        "from google.genai.types import HttpOptions\n",
        "\n",
        "\n",
        "# --- Configure the Gemini client ---\n",
        "client = genai.Client(vertexai=True, project='remy-sandbox', location='us-central1', http_options=HttpOptions(api_version=\"v1\"))\n",
        "MODEL_ID = \"gemini-2.5-flash\"\n",
        "\n",
        "\n",
        "# --- Tool Implementation: Google Drive search ---\n",
        "def search_google_drive(\n",
        "    query: str,\n",
        "    max_results: int = 10,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Execute a Google Drive search and return a JSON-serializable payload.\n",
        "    Query string is used to search for file names.\n",
        "    \"\"\"\n",
        "    creds, _ = default()\n",
        "    drive_service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "    results = drive_service.files().list(\n",
        "        q=f\"name contains '{query}' and mimeType != 'application/vnd.google-apps.folder'\",\n",
        "        pageSize=max_results,\n",
        "        fields=\"nextPageToken, files(id, name, mimeType, webViewLink)\"\n",
        "    ).execute()\n",
        "\n",
        "    items = results.get('files', [])\n",
        "    \n",
        "    files_content = []\n",
        "    for item in items:\n",
        "        file_id = item['id']\n",
        "        file_name = item['name']\n",
        "        mime_type = item['mimeType']\n",
        "        \n",
        "        content = None\n",
        "        try:\n",
        "            if mime_type == 'application/vnd.google-apps.document': # Google Doc\n",
        "                request = drive_service.files().export_media(fileId=file_id, mimeType='text/plain')\n",
        "                content = request.execute().decode('utf-8')\n",
        "            elif mime_type.startswith('text/') or mime_type == 'application/json' or mime_type == 'application/rtf' or mime_type == 'application/pdf':\n",
        "                 if mime_type == 'application/pdf':\n",
        "                     # can't just read pdfs, would need a library. Skipping for now.\n",
        "                     continue\n",
        "                 request = drive_service.files().get_media(fileId=file_id)\n",
        "                 content = request.execute().decode('utf-8')\n",
        "        except Exception as e:\n",
        "            print(f'Error reading file {file_name}: {e}')\n",
        "            content = f'Error reading file: {e}'\n",
        "\n",
        "        if content:\n",
        "            files_content.append({\n",
        "                \"name\": file_name,\n",
        "                \"summary\": content[:800], # Truncate for brevity\n",
        "                \"link\": item['webViewLink']\n",
        "            })\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"count\": len(files_content),\n",
        "        \"results\": files_content,\n",
        "    }\n",
        "\n",
        "# --- Tool declaration (matches Gemini function-calling schema) ---\n",
        "gdrive_tool = types.Tool(\n",
        "    function_declarations=[\n",
        "        {\n",
        "            \"name\": \"search_google_drive\",\n",
        "            \"description\": (\n",
        "                \"Search Google Drive for documents with a given query in their name. \"\n",
        "                \"Returns a JSON list of documents with metadata and content.\"\n",
        "            ),\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Query string to search for in file names in Google Drive.\",\n",
        "                    },\n",
        "                    \"max_results\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"minimum\": 1,\n",
        "                        \"maximum\": 50,\n",
        "                        \"default\": 10,\n",
        "                        \"description\": \"Maximum number of files to return (cap at 50). ignited by users ask\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"query\"],\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --- Dispatch: map function calls to Python implementations ---\n",
        "def handle_tool_call(name: str, args: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    if name == \"search_google_drive\":\n",
        "        return search_google_drive(**args)\n",
        "    raise ValueError(f\"Unknown tool: {name}\")\n",
        "\n",
        "config = types.GenerateContentConfig(tools=[gdrive_tool])\n",
        "# --- Conversation with function calling loop ---\n",
        "def ask_gemini_with_gdrive(prompt: str, model: str = \"gemini-2.5-flash\") -> str:\n",
        "    contents = [\n",
        "        types.Content(role=\"user\", parts=[ types.Part(text=prompt) ])\n",
        "    ]\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=contents,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    tool_call = None\n",
        "    for cand in response.candidates or []:\n",
        "        for part in (cand.content.parts or []):\n",
        "            if getattr(part, \"function_call\", None):\n",
        "                tool_call = part.function_call\n",
        "                break\n",
        "        if tool_call:\n",
        "            break\n",
        "\n",
        "    if not tool_call:\n",
        "        return response.text\n",
        "\n",
        "    args = tool_call.args\n",
        "    tool_result = handle_tool_call(tool_call.name, args)\n",
        "\n",
        "    function_response_part = types.Part.from_function_response(\n",
        "        name=tool_call.name,\n",
        "        response={\"result\": tool_result},\n",
        "    )\n",
        "\n",
        "    contents.append(response.candidates[0].content)\n",
        "    contents.append(types.Content(role=\"user\", parts=[function_response_part]))\n",
        "\n",
        "    final = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=contents,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    return final.text\n",
        "\n",
        "# --- Example usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    demo_prompt = (\n",
        "        \"Find 5 of my stories in my Google Drive about 'AI'. Summarize them.\"\n",
        "    )\n",
        "    print(ask_gemini_with_gdrive(demo_prompt))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC105_paXsFj"
      },
      "source": [
        "#Build an autonomous system for generating newsletters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmNjaAVSXgpr"
      },
      "outputs": [],
      "source": [
        "problem = \"\"\"You are the curator for the Agentville newsletter. Your task is to assemble the content for the centennial edition, covering the developments in autonomous agents. The newsletter should be in a witty format\n",
        "and should cover different dimensions of autonomous agents.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snVFjqW-X63s"
      },
      "source": [
        "##Planner in action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14fAzU1UypyI"
      },
      "outputs": [],
      "source": [
        "plan = model_response(f'''You are a planning agent inside an autonomous multi-agent system.\\n",
        "Your job is to take a user's goal:{problem} and break it into a structured to-do list of clear steps.\\n",
        "You have access to Search and Google Drive retrieval tools.\\n",
        "\\n",
        "Instructions:\\n",
        "1. Understand the user’s goal.\\n",
        "2. Break it down into the smallest actionable steps needed to achieve it. You are not supposed to come up with\\n",
        "the final answer to the problem.\\n",
        "3. Each step must be atomic (can be completed by a single specialized agent).\\n",
        "4. Order the steps logically. Outline which agent among search and gdrive needs to be used to tackle each step.\\n",
        "5.Format of the step: Tool name, step description. Available tool names are agent_search and agent_gdrive\\n",
        "6.Separate each step with a --\\n",
        "7. Include a final step to \"summarize the overall findings\" once all tasks are done.''',\"gemini-2.5-pro\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKs3cErb0Ypp",
        "outputId": "cd9672ef-c728-4227-ca06-3c4643821f35"
      },
      "outputs": [],
      "source": [
        "print (plan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZfB1PhUy0RB"
      },
      "outputs": [],
      "source": [
        "plan_steps = plan.split('--')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzMQ2uWiYT8U"
      },
      "source": [
        "## Helper functions to call Explorer and Scholar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFrdP-t65Ufc"
      },
      "outputs": [],
      "source": [
        "def handle_search(memory, step: str):\n",
        "    print(f\"[SEARCH] Handling search step: {step}\")\n",
        "    r = search_chat.send_message(f'''For the step: {step}, extract relevant context from memory: {memory} and execute the step.''')\n",
        "    response = show_parts(r)\n",
        "    return response\n",
        "\n",
        "def handle_gdrive(memory, step: str):\n",
        "    print(f\"[GDRIVE] Handling gdrive step: {step}\")\n",
        "    response = ask_gemini_with_gdrive(f'''Find out documents from google drive corresponding to the {step}. Use {memory} for context.''')\n",
        "    return response\n",
        "\n",
        "def handle_default(memory, step: str):\n",
        "    print(f\"[DEFAULT] Handling generic step: {step}\")\n",
        "    response = model_response(f'''For the step: {step}, extract relevant context from memory: {memory} and execute the step.''',\"gemini-2.5-flash\")\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQtIPoQNYjId"
      },
      "source": [
        "##Agent orchestration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bsLKePt5yW-8",
        "outputId": "2201a0dc-9987-4d35-d3eb-4ad26646bf61"
      },
      "outputs": [],
      "source": [
        "memory = {}\n",
        "for i in range(1,len(plan_steps)+1):\n",
        "  print (f\"{plan_steps[i-1]}\")\n",
        "  if \"agent_search\" in plan_steps[i-1]:\n",
        "        agent_response = handle_search(memory, plan_steps[i-1])\n",
        "  elif \"agent_gdrive\" in plan_steps[i-1]:\n",
        "        agent_response = handle_gdrive(memory, plan_steps[i-1])\n",
        "  else:\n",
        "        agent_response = handle_default(memory, plan_steps[i-1])\n",
        "  memory[plan_steps[i-1]] = agent_response\n",
        "  print (memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikxWOmlONGeH",
        "outputId": "10b1f544-0099-48c7-a6a1-8ad317aee1da"
      },
      "outputs": [],
      "source": [
        "print (memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuKyrTlCVs85",
        "outputId": "91f0dcfe-5ef5-4910-9642-33c5c872472e"
      },
      "outputs": [],
      "source": [
        "print (model_response(f'''Render the response:{memory} in the form of a detailed report in markdown format with proper indentation.\n",
        "Preserve the hyperlinks to the research papers and links cited.''','gemini-2.5-flash'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJXAQyVWOdV0"
      },
      "source": [
        "## Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4ypXXGZOKvr"
      },
      "outputs": [],
      "source": [
        "critic_response = model_response(f'''For the given problem:{problem}, a bunch of agents worked together to curate the response: {memory}. Your\n",
        "job is to analyze how well the original plan: {plan} was executed and suggest improvements for the overall system and individual agents involved.''',\"gemini-2.5-pro\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8h3WF7rUdJ3",
        "outputId": "26b3fa95-6649-4fa6-ca50-6131b6a9e98f"
      },
      "outputs": [],
      "source": [
        "print (critic_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZu5htAmuYZI"
      },
      "source": [
        "# Bringing everything together: Autonomous Agentic Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdW3DdYYUv-c"
      },
      "outputs": [],
      "source": [
        "def autonomous_newsletter_generation(problem):\n",
        "  plan = model_response(f'''You are a planning agent inside an autonomous multi-agent system.\\n",
        "Your job is to take a user's goal:{problem} and break it into a structured to-do list of clear steps.\\n",
        "You have access to Search and Google Drive retrieval tools.\\n",
        "\\n",
        "Instructions:\\n",
        "1. Understand the user’s goal.\\n",
        "2. Break it down into the smallest actionable steps needed to achieve it. You are not supposed to come up with\\n",
        "the final answer to the problem.\\n",
        "3. Each step must be atomic (can be completed by a single specialized agent).\\n",
        "4. Order the steps logically. Outline which agent among search and gdrive needs to be used to tackle each step.\\n",
        "5.Format of the step: Tool name, step description. Available tool names are agent_search and agent_gdrive\\n",
        "6.Separate each step with a --\\n",
        "7. Include a final step to \"summarize the overall findings\" once all tasks are done.''',\"gemini-2.5-pro\")\n",
        "  print (\"Plan is\",plan)\n",
        "  plan_steps = plan.split('--')\n",
        "  memory = {}\n",
        "  for i in range(1,len(plan_steps)+1):\n",
        "     print (f\"{plan_steps[i-1]}\")\n",
        "     if \"agent_search\" in plan_steps[i-1]:\n",
        "        agent_response = handle_search(memory, plan_steps[i-1])\n",
        "     elif \"agent_gdrive\" in plan_steps[i-1]:\n",
        "        agent_response = handle_gdrive(memory, plan_steps[i-1])\n",
        "     else:\n",
        "        agent_response = handle_default(memory, plan_steps[i-1])\n",
        "     memory[plan_steps[i-1]] = agent_response\n",
        "\n",
        "  agent_report = model_response(f'''Render the response:{memory} in the form of a detailed report in markdown format with proper indentation.\\n",
        "Preserve the hyperlinks to the documents and links cited.''','gemini-2.5-flash')\n",
        "  critic_response = model_response(f'''For the given problem:{problem}, a bunch of agents worked together to curate the response: {memory}. Your\\n",
        "job is to analyze how well the original plan: {plan} was executed and suggest improvements for the overall system and individual agents involved.''',\"gemini-2.5-pro\")\n",
        "  return agent_report, critic_response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9op0bzUqvcol",
        "outputId": "ca6d5e81-fe61-4a5f-c483-d7fc0b271822"
      },
      "outputs": [],
      "source": [
        "report, analysis = autonomous_newsletter_generation('''Generate a newsletter around memory techniques used for\n",
        "autonomous agentic systems''')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "agents",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}